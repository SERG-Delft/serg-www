---
title: "Master Thesis Defense on Studying the ML Lifecycle and Improving Code Quality of ML Applications"

# event: 
# event_url: 

location: Zoom

summary: ""
# abstract: ""

# Talk start and end times.
# End time can optionally be hidden by prefixing the line with `#`.
date: "2020-07-22T10:00:00Z"
date_end: "2020-07-22T12:00:00Z"
all_day: false

# Schedule page publish date (NOT talk date).
# publishDate:

authors: [Mark Haakman]
tags: [events,defenses]

# Is this a featured talk? (true/false)
featured: true

# Adding an image and image caption
# image:
# caption: 
# focal_point: Right

url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: []

# Projects (optional).
#   Associate this post with one or more projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []

---


As organizations start to adopt machine learning in critical business
scenarios, the development processes change and the reliability of the
applications becomes more important. To investigate these changes and improve
the reliability of those applications, we conducted two studies in this thesis.
The first study aims to understand the evolution of the processes by which
machine learning applications are developed and how state-of-the-art lifecycle
models fit the current needs of the fintech industry. Therefore, we conducted a
case study with seventeen machine learning practitioners at the fintech company
ING. The results indicate that the existing lifecycle models CRISP-DM and TDSP
largely reflect the current development processes of machine learning
applications, but there are crucial steps missing, including a feasibility
study, documentation, model evaluation, and model monitoring. Our second study
aims to reduce bugs and improve the code quality of machine learning
applications. We developed a static code analysis tool consisting of six
checkers to find probable bugs and enforcing best practices, specifically in
Python code used for processing large amounts of data and modeling in the
machine learning lifecycle. The evaluation of the tool using 1000 collected
notebooks from Kaggle shows that static code analysis can detect and thus help
prevent probable bugs in data science code. Our work shows that the real
challenges of applying machine learning go much beyond sophisticated learning
algorithms -- more focus is needed on the entire lifecycle.

Thesis: [link](http://resolver.tudelft.nl/uuid:38ff4e9a-222a-4987-998c-ac9d87880907)

