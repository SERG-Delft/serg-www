---
title: "SERG talk"

# event: 
# event_url: 

location: Online

summary: ""
# abstract: ""

# Talk start and end times.
# End time can optionally be hidden by prefixing the line with `#`.
date: "2021-03-31T11:00:00Z"
date_end: "2021-03-31T12:00:00Z"
all_day: false

# Schedule page publish date (NOT talk date).
# publishDate:

authors: [Carolin Brandt]
tags: [events,lunch-talks]

# Is this a featured talk? (true/false)
featured: true

# Adding an image and image caption
# image:
# caption: 
# focal_point: Right

url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Markdown Slides (optional).
#   Associate this talk with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: []

# Projects (optional).
#   Associate this post with one or more projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []

---


**Topic 1: Developer-Friendly Test Amplification: The Interplay between Automatic Generation and Human Exploration (Carolin Brandt)**

Caro gives some insights on her first PhD project & we can learn from it:

Automatically generating test cases for software is an active research topic for many years. However, the powerful tools we create are up until now rarely used in practice. Starting from issues reported by developers in previous studies, we investigate what aspects are important to design test generation approaches that developers appreciate. We conduct 16 semi-structured interviews with software developers supported by our prototypical designs of developer-friendly test amplification and exploration tools. We extend the test amplification tool DSpot, generating test cases that are easier to understand. The IntelliJ plugin TestCube empowers developers to explore generated test cases from their familiar environment. Our designs are based on our vision of putting the developer more tightly into the loop, paving the way for test generation tools to benefit from the developerâ€™s knowledge in the future. From our interviews, we gather 52 observations that we summarize into 23 results and give three key recommendations on how current and future test generation tools should be designed so that developers appreciate using them.

